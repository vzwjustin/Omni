# Omni-Cortex Configuration
# Architecture: Gemini routes, Claude Code executes

# =============================================================================
# ARCHITECTURE OVERVIEW
# =============================================================================
# Gemini (cheap) → Task analysis, routing, execution plans
# Claude Code (local) → Receives rich context, does actual reasoning/coding
#
# This saves tokens: Gemini handles the "thinking about thinking" so Claude
# can focus on execution with rich pre-analyzed context.

# =============================================================================
# ROUTING MODEL (Required - Gemini for task analysis)
# =============================================================================
GOOGLE_API_KEY=your_google_api_key_here

# Model for routing/analysis (Gemini 3 Flash is fast & cheap)
ROUTING_MODEL=gemini-3-flash-preview

# =============================================================================
# EMBEDDINGS (for ChromaDB / RAG)
# =============================================================================
EMBEDDING_PROVIDER=openrouter
EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_API_KEY=your_openai_api_key_here  # If using OpenAI embeddings

# =============================================================================
# CHROMADB STORAGE
# =============================================================================
CHROMA_PERSIST_DIR=/app/data/chroma

# =============================================================================
# FEATURE FLAGS
# =============================================================================
ENABLE_AUTO_INGEST=true
ENABLE_DSPY_OPTIMIZATION=true
ENABLE_PRM_SCORING=true

# =============================================================================
# MCP TOOL EXPOSURE
# =============================================================================
# LEAN_MODE=true (default) → Only expose 'reason' + utilities (14 tools)
#   HyperRouter handles framework selection internally
#   Saves ~55k tokens in MCP tool definitions
# LEAN_MODE=false → Expose all 55+ think_* tools individually
LEAN_MODE=true

# =============================================================================
# EXECUTION MODE
# =============================================================================
# USE_LANGCHAIN_LLM=false → Template mode (Claude Code executes)
# USE_LANGCHAIN_LLM=true → Server calls LLM (burns API tokens)
USE_LANGCHAIN_LLM=false

# LLM_PROVIDER only used if USE_LANGCHAIN_LLM=true
# Leave as pass-through for Claude Code execution
LLM_PROVIDER=pass-through

# =============================================================================
# LIMITS & TUNING
# =============================================================================
MAX_REASONING_DEPTH=10
MCTS_MAX_ROLLOUTS=50
DEBATE_MAX_ROUNDS=5

# =============================================================================
# LOGGING
# =============================================================================
LOG_LEVEL=INFO
