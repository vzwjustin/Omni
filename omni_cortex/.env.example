# Omni-Cortex Configuration
# Pass-through mode - calling LLM does the reasoning

# =============================================================================
# MODE
# =============================================================================
# pass-through: MCP returns prompts, calling LLM executes
LLM_PROVIDER=pass-through

# =============================================================================
# EMBEDDINGS (for ChromaDB / RAG)
# =============================================================================
# Embedding provider: openrouter, openai, or huggingface
EMBEDDING_PROVIDER=openrouter

# Embedding model (OpenRouter/OpenAI compatible models)
EMBEDDING_MODEL=text-embedding-3-small

# API key for embeddings (uses same key as LLM if using same provider)
# OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# CHROMADB STORAGE
# =============================================================================
CHROMA_PERSIST_DIR=/app/data/chroma

# =============================================================================
# FEATURE FLAGS
# =============================================================================
# Automatically ingest repo into Chroma at startup
ENABLE_AUTO_INGEST=true

# DSPy-style prompt optimization
ENABLE_DSPY_OPTIMIZATION=true

# Process Reward Model for scoring reasoning steps
ENABLE_PRM_SCORING=true

# =============================================================================
# LLM FALLBACK (Optional - for direct API calls)
# =============================================================================
# When true, uses LangChain to call LLMs directly instead of template mode
# This costs $$$ but provides actual multi-turn reasoning
USE_LANGCHAIN_LLM=true

# Provider for direct LLM calls (when USE_LANGCHAIN_LLM=true)
# Options: google, openrouter, anthropic, openai
LLM_PROVIDER=google

# Google AI API key (recommended - direct Gemini access, cheapest)
GOOGLE_API_KEY=your_google_api_key_here

# Or use OpenRouter (single key for all models):
# LLM_PROVIDER=openrouter
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Or use direct provider keys:
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Model selection (defaults to Gemini 3 Flash - fastest & cheapest)
DEEP_REASONING_MODEL=gemini-3-flash-preview
FAST_SYNTHESIS_MODEL=gemini-3-flash-preview

# Alternative models:
# DEEP_REASONING_MODEL=gemini-2.5-pro              # Google AI (higher quality)
# DEEP_REASONING_MODEL=anthropic/claude-sonnet-4-20250514  # OpenRouter

# =============================================================================
# LIMITS & TUNING
# =============================================================================
# Maximum depth for recursive reasoning frameworks
MAX_REASONING_DEPTH=10

# MCTS maximum rollouts per search
MCTS_MAX_ROLLOUTS=50

# Multi-agent debate maximum rounds
DEBATE_MAX_ROUNDS=5

# =============================================================================
# LOGGING
# =============================================================================
LOG_LEVEL=INFO
